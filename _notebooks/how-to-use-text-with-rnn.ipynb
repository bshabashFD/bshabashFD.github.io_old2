{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "RNN poetry.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOFKvQEfS4k_",
        "colab_type": "text"
      },
      "source": [
        "# Employing Recurrent Neural Networks for Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3NYxyMSS0jd",
        "colab_type": "text"
      },
      "source": [
        "Recurrent Neural Networks (RNNs) are naturally designed to process sequences and extract patterns from them. Unlike Feed-Forward NNs which accept simply a collection of inputs as features, RNNs can pay attention to the order of the inputs coming in, and as such are often used to explore patterns through time or through any naturally occurring sequence.\n",
        "\n",
        "In this post, we’ll explore how to employ RNNs for text processing and generation, and we’ll be using TensorFlow and its Keras API. This post is loosely based on the tutorial at https://www.tensorflow.org/beta/tutorials/text/text_generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttUKMcAmSx0z",
        "colab_type": "text"
      },
      "source": [
        "## Phase 1: Obtain the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAP1GlQPao8r",
        "colab_type": "text"
      },
      "source": [
        "Lucky for us, Keras comes shipped with an example text data. Let’s import everything we need, and grab our text data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gImvfk7TNEj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "ad3a4342-8c68-47c4-fd06-63707b92c9a6"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcWkuG6Bao8u",
        "colab_type": "code",
        "outputId": "ac89c2e9-7e20-41d3-ad2e-43f9c861812c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print('length of text: {} chars'.format(len(text)))\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique chars'.format(len(vocab)))\n",
        "\n",
        "print(text[:400])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "length of text: 1115394 chars\n",
            "65 unique chars\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pDEK5_qThb7",
        "colab_type": "text"
      },
      "source": [
        "Ahhh, that’s nice, some works of William Shakespeare. I’m not sure if it’s all of them (he did write a lot), but we have The Tragedy of Coriolanus, and The Tempest, to name a few, in there.\n",
        "\n",
        "Now that we have the data, what can we do with it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpCnSkIwao8v",
        "colab_type": "text"
      },
      "source": [
        "## Phase 2: Define our problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLRsqM07TnYe",
        "colab_type": "text"
      },
      "source": [
        "We’re going to use this text collection to train an RNN which can generate Shakespeare-like writing. We will give it a string of length $n$ characters, and it will generate the next character for us. We see we have 65 unique characters, so this will be a 65 class classification problem. We will provide a single string of length $n$, and will process it to create an output of 1 character.\n",
        "\n",
        "Simply speaking, we’d like our network to learn to print “o” when we provide the string “hell” (to make “hello”) or “e” when we provide the string “goodby” (to make “goodbye”).\n",
        "\n",
        "let’s define our string length to be 100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTzLWYvwTwKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_length = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrILmVj5TzSo",
        "colab_type": "text"
      },
      "source": [
        "## Phase 3: Pre-process our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4p0JFNFTzU7",
        "colab_type": "text"
      },
      "source": [
        "RNNs, and really all machine learning methods, can only accept numbers as inputs, so we need first to number’ify our text. What this means is that we will assign every character to a number, and also have a way of converting those numbers back to characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ThCadcUao8w",
        "colab_type": "code",
        "outputId": "5be0e120-1a01-4f71-8664-4d86f07691eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# dictionary comprehension, assign every character to a number from 0-64\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# let’s check our text after it’s been transformed into numbers\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "print(text_as_int)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18 47 56 ... 45  8  0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DfARLJwUGIC",
        "colab_type": "text"
      },
      "source": [
        "Now we have a long array with numbers in it, we want to create a dataset which has a 100 number long sequence as the input (X), and a single number as the output (Y). We’re going to use a python deque for that, which is like a list but has limited capacity, so that when it fills up, and we enter another element, the oldest element is kicked out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNkuh19mao8z",
        "colab_type": "code",
        "outputId": "a9b95b41-74f9-4534-fb76-afce19d8a666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "newline_idx = char2idx[\"\\n\"]\n",
        "\n",
        "# Initialize a data deque with only a newline in it, that takes care of the fact the first line of the \n",
        "# text isn't preceeded by a newline, but every other new line in the text is\n",
        "\n",
        "data_deque = deque([newline_idx],maxlen=seq_length)\n",
        "\n",
        "\n",
        "for i, char_id in enumerate(text_as_int[:-1]):\n",
        "    data_deque.append(char_id)\n",
        "    \n",
        "    if (len(data_deque) == seq_length):\n",
        "        X_data.append(list(data_deque))\n",
        "        y_data.append(text_as_int[i+1])\n",
        "    \n",
        "    if ((i % 100) == 0):\n",
        "        print(i, end=\"\\r\")\n",
        "\n",
        "print(i)\n",
        "\n",
        "X_data_np = np.array(X_data)\n",
        "y_data_np = np.array(y_data)\n",
        "\n",
        "print(X_data_np.shape)\n",
        "print(y_data_np.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1115392\n",
            "(1115295, 100)\n",
            "(1115295,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_iSZ2aeUQ8O",
        "colab_type": "text"
      },
      "source": [
        "Let’s take a look at out created datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpPs_Y5Iao81",
        "colab_type": "code",
        "outputId": "5ef582cc-fd80-4cd7-dd7a-0ccf8d9a4aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# Let's take a look at the X and Y data\n",
        "for i in range(5):\n",
        "    print(\"X:\\t\",repr(''.join(idx2char[X_data_np[i]])))\n",
        "    print(\"y:\\t\",repr(idx2char[y_data_np[i]]))\n",
        "    print('-------------')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X:\t '\\nFirst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYo'\n",
            "y:\t 'u'\n",
            "-------------\n",
            "X:\t 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "y:\t ' '\n",
            "-------------\n",
            "X:\t 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "y:\t 'a'\n",
            "-------------\n",
            "X:\t 'rst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou a'\n",
            "y:\t 'r'\n",
            "-------------\n",
            "X:\t 'st Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ar'\n",
            "y:\t 'e'\n",
            "-------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK8PT2VaUXEU",
        "colab_type": "text"
      },
      "source": [
        "Looks good, the last thing we’re going to do, is shuffle the data around. We’re going to shuffle the data again later on, so let’s make a function to do that for us"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5LD-D7Zao82",
        "colab_type": "code",
        "outputId": "13fbc5b1-112d-4541-87db-010a110a0be9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "def shuffle_data(X_data, y_data):\n",
        "    \n",
        "    y_data = y_data.reshape((y_data.shape[0], 1))\n",
        "    combined_data = np.hstack((X_data, y_data))\n",
        "    \n",
        "    np.random.shuffle(combined_data)\n",
        "\n",
        "    X_data = combined_data[:, :-1]\n",
        "    y_data = combined_data[:, -1]\n",
        "    \n",
        "    return X_data, y_data\n",
        "\n",
        "\n",
        "X_data_np, y_data_np = shuffle_data(X_data_np, y_data_np)\n",
        "print(X_data_np.shape)\n",
        "print(y_data_np.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1115295, 100)\n",
            "(1115295,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhjQt6gbUbOU",
        "colab_type": "text"
      },
      "source": [
        "## Phase 4: Build and Compile our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjNa42okUgcI",
        "colab_type": "text"
      },
      "source": [
        "Finally we can create our model and train it. We’re going to use Long-Short Term Memory units (LSTM units) as our recurrent units, but you can experiment with Gated Recurrent Units (GRUs) as well.\n",
        "\n",
        "But before we can build our model, we need to consider one more thing. Recall we just assigned a number between 0 and 64 to each of our 65 characters. This was an easy way to turn those text characters into numbers, but we ended up imposing an artificial order on them. We could figure out some representation which is more meaningful, but we can also have Keras do it for us by introducing an Embedding Unit. In essence, an embedding unit of n dimensions, takes all our alphabet (in this case 65 characters) and learns a unique vector representation for each character in this n-dimensional space. In our case, we will take our characters and convert each into a 25 dimensional vector.\n",
        "\n",
        "Let’s build our Recurrent Neural Network:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npkAHZ0Sao86",
        "colab_type": "code",
        "outputId": "dc445d20-2477-49fa-83c3-4c4397f0ebed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(len(vocab), 25, input_length=seq_length))\n",
        "model.add(LSTM(1024))\n",
        "model.add(Dense(len(vocab), activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', \n",
        "              optimizer='adam')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0804 17:56:57.496224 139674278631296 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0804 17:56:57.535739 139674278631296 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 25)           1625      \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 1024)              4300800   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 65)                66625     \n",
            "=================================================================\n",
            "Total params: 4,369,050\n",
            "Trainable params: 4,369,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILXhENTXUpsD",
        "colab_type": "text"
      },
      "source": [
        "Let’s review what we have there:\n",
        "\n",
        "First, an embedding input layer, it takes a 100 character long sequence and converts each character into a 25 dimensional vector. Thus the output of this layer is a 100$\\times$25 two dimensional matrix.\n",
        "\n",
        "Then, we have a 1024 neuron LSTM layer, it takes the 100$\\times$25 matrix and produces a 1024 single set of values as output. Each LSTM neuron reads the 100$\\times$25 matrix, learns to make sense of the sequence of 100 25-dimensional vectors, and outputs a single value, creating a total of 1024 values being passed on to the final layer.\n",
        "\n",
        "The final layer is a simple layer of 65 neurons, who each take the 1024-long output of the 2nd layer, and outputs a number from 0.0-1.0. Each of the 65 final neurons correspond to one of the 65 characters, so each $i^{th}$ neuron outputs the probability the output character for the input sequence is character $i$ (i.e. neuron 0 outputs the probability the next character in the sequence is character 0, neuron 1 outputs the probability the next character in the sequence is character 1, etc’).\n",
        "\n",
        "Once the model is declared, we compile it to bring it to life, and we output the summary to get an idea of our model’s architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bo5rQasUx-C",
        "colab_type": "text"
      },
      "source": [
        "## Phase 5: Training and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Bqm6RVNU044",
        "colab_type": "text"
      },
      "source": [
        "We will train our RNN for 5 epochs, which means it will look at the data 5 times, but to save our poor RAM memory from exploding, we will do so in batches of 480. This means we will divide out dataset into chunks of 480 data points, and show each chunk to the model. Once all chunks have been shown, we will call it an epoch. We’ll then repeat the process 4 more times.\n",
        "\n",
        "However, after each epoch, we’d like to see what the model has learned, so let’s define a function that will take our model, an input set of characters, and produce an output of a given size:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXnsX3ozao88",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_text_from_model(model, test_input, output_size=150):\n",
        "    \n",
        "    combined_text = test_input+\"\"\n",
        "    \n",
        "    for i in range(output_size):\n",
        "        if (len(test_input) > seq_length):\n",
        "            padded_test_input = test_input[-seq_length:]\n",
        "        else:\n",
        "            padded_test_input = (\"\\n\" * (seq_length - len(test_input))) + test_input\n",
        "        text_as_int = np.array([char2idx[c] for c in padded_test_input])\n",
        "        text_as_int = text_as_int.reshape((1, seq_length))\n",
        "\n",
        "        y_predict_proba = model.predict(text_as_int)[0]\n",
        "        y_id = np.random.choice(list(range(len(vocab))), size=1, p=y_predict_proba)[0]\n",
        "\n",
        "        y_char = idx2char[y_id]\n",
        "        test_input = test_input+y_char\n",
        "\n",
        "    return test_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F6K8loYU77s",
        "colab_type": "text"
      },
      "source": [
        "This function basically takes our model, a seed input, and a length, and produces an output of that length with our seed input as the initial set of characters. You can see later how this works.\n",
        "\n",
        "Let’s train our model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMSgAQs0ao9B",
        "colab_type": "code",
        "outputId": "ce5456c5-128f-4ca1-dc4e-df6b3592ac94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 978
        }
      },
      "source": [
        "EPOCHS = 5       # NNs operate in epochs, meaning this is how many times the neural network will go \n",
        "                 # through the entire data\n",
        "BATCH_SIZE = 480   # at each epoch, it will split the data into units of 480 samples, and train on those\n",
        "\n",
        "\n",
        "for i in range(1, EPOCHS+1):\n",
        "  print(\"EPOCH: \",i)\n",
        "  X_data_np, y_data_np = shuffle_data(X_data_np, y_data_np)\n",
        "  model.fit(X_data_np, y_data_np,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            epochs=1)\n",
        "\n",
        "  test_input = \"ROMEO:\"\n",
        "  print(get_text_from_model(model, test_input, output_size=150))\n",
        "  print(\"------------\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EPOCH:  1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0804 17:57:07.402892 139674278631296 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1115295/1115295 [==============================] - 1401s 1ms/sample - loss: 1.9630\n",
            "ROMEO:\n",
            "Field I'll veny soldower thro! he tell me sich '\n",
            "Masten fastiching queen by them give good now.\n",
            "\n",
            "KENG EDWARD IV:\n",
            "I loud be Plarence and love that wit\n",
            "------------\n",
            "EPOCH:  2\n",
            "1115295/1115295 [==============================] - 1396s 1ms/sample - loss: 1.3981\n",
            "ROMEO:\n",
            "The skning Say,---\n",
            "\n",
            "ANHID:\n",
            "Thy swords, pierces, night, the Bodn the stature.\n",
            "\n",
            "Gardee:\n",
            "Howsoe't you at no ordete? Yis fings of hid?\n",
            "What, willingly ha\n",
            "------------\n",
            "EPOCH:  3\n",
            "1115295/1115295 [==============================] - 1394s 1ms/sample - loss: 1.2618\n",
            "ROMEO:\n",
            "We cannot sheak fairly than an enamish them, can Edward\n",
            "it were to-ingorts: stere my sound and his\n",
            "behadd; and a'l siveness, so shalt some of all.\n",
            "\n",
            "D\n",
            "------------\n",
            "EPOCH:  4\n",
            "1115295/1115295 [==============================] - 1393s 1ms/sample - loss: 1.1783\n",
            "ROMEO:\n",
            "Some son, what say'st thou to her in your good!\n",
            "\n",
            "ANGELO:\n",
            "Before a whor, would think on the holy babe?\n",
            "If when I read; she shall here weap and wonder\n",
            "\n",
            "------------\n",
            "EPOCH:  5\n",
            "1115295/1115295 [==============================] - 1391s 1ms/sample - loss: 1.1100\n",
            "ROMEO:\n",
            "Me, Signior Gremio.\n",
            "\n",
            "BRUTUS:\n",
            "And give me packing, what, his ,\n",
            "Warwick's heavy's ard.\n",
            "\n",
            "MARIANA:\n",
            "My lord, must make his looks ashect my other so:\n",
            "Later\n",
            "------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHq5PsqyVG5W",
        "colab_type": "text"
      },
      "source": [
        "By the end of the first epoch, our model learned the syntax for Shakespearean writing, but it still produces some garbage language with only short words resembling English (even Shakespearean English). However, by the end of epoch 5, our model produces mostly coherent words. Furthermore, since we parsed our text as a set of characters, we can even give our model names and words which do not appear in the text, and it can handle them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFYUF40Gao9T",
        "colab_type": "code",
        "outputId": "fbdc89f2-0435-46dd-e62d-a82365ec0706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "print(get_text_from_model(model, \"BORIS THE BLADE:\", output_size=450))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BORIS THE BLADE:\n",
            "And stay, lady Angelo;\n",
            "When you should come.\n",
            "\n",
            "ISABELLA:\n",
            "I' child;\n",
            "Then is the white will seem to my sworn with\n",
            "York warmons more, which I repuse mine ear still.\n",
            "\n",
            "GRUMIO:\n",
            "Grumbed, might talk: Yeld you ang royal prisoners, neither\n",
            "my ways; but what a master is't, that I\n",
            "\n",
            "ESCALUS:\n",
            "I care no one thing it for the people.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "What, think you, this on my patient witch as ever\n",
            "to his bodies? when he be trusted and progector,\n",
            "Were he for my \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-PvzoHVNsr",
        "colab_type": "text"
      },
      "source": [
        "Well, there we have it. We could probably train our model further and make it produce more and more English like text, but let’s stop and think for a second, do we really need to generate our text character by character? We can actually make the process much easier for our model and simply split the text into individual words rather than individual characters."
      ]
    }
  ]
}
